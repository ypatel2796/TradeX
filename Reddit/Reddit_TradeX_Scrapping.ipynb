{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "175a2893",
   "metadata": {},
   "source": [
    "# IMT 575 C - Group Project - TradeX\n",
    "\n",
    "## Module: Reddit_TradeX_Scrapping.py Ver.01\n",
    "\n",
    "### Durga Prasad Tavva\n",
    "\n",
    "#### 05/06/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de7f1e8",
   "metadata": {},
   "source": [
    "#### Load Reddit Scrapper Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cad32fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load reddit_scraper_module.py\n",
    "\"\"\"This module contains helper functions for the PushShift API object.\n",
    "   The functions in thiss module help a user taken in a PushShift API\n",
    "   return object, clean the values, and return a dataframe with the\n",
    "   cleaned values as well as date, and stock ticker values as separate\n",
    "   rows.\n",
    "\n",
    "   Returns:\n",
    "   [Pandas DataFrame]: DataFrame object cleaned reddit submissions.\"\"\"\n",
    "import datetime as dt\n",
    "import re\n",
    "\n",
    "class RedditData:\n",
    "    '''\n",
    "    This class contains helper functions that parse through a PushShift API object to return a\n",
    "    cleaned dataframe with separated Date-wise Tags and Comment values.\n",
    "    '''\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    @staticmethod\n",
    "\n",
    "    def has_numbers(input_string):\n",
    "        '''\n",
    "        This function looks at the $Tag and checks if it contains a dollar value or a ticker symbol.\n",
    "        For eg. is it $9 or $AMC. Returns True if it is a number and false if it is a string.\n",
    "        '''\n",
    "        return any(char.isdigit() for char in input_string)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_subreddit_column(subreddit, lst_tags):\n",
    "        '''\n",
    "        This function takes the channel variable and the dataframe of api results as input\n",
    "        and adds a column in the same dataframe of the length of the dataframe and populates\n",
    "        it with the name of the subreddit (channel variable) name.\n",
    "        '''\n",
    "        arr_subreddit = []\n",
    "        size = len(lst_tags)\n",
    "        arr_subreddit += size * [str(subreddit)]\n",
    "        return arr_subreddit\n",
    "\n",
    "    @staticmethod\n",
    "    def cashtags(submissions):\n",
    "        '''\n",
    "        This function takes the api JSON values and the dataframe. It extracts the $Tags from the\n",
    "        comments and stores them in a separate column, corresponding to the row of the comment.\n",
    "        It also filters out the $Tags that are empty and those that have a $Numerical value.\n",
    "        Returns a list of tags and a list of comments.\n",
    "        '''\n",
    "        #pylint: disable=too-many-locals\n",
    "        tag_list = []\n",
    "        comment_list = []\n",
    "        date_list = []\n",
    "        subreddit_list = []\n",
    "        lst_tag = []\n",
    "        lst_comments = []\n",
    "        lst_date = []\n",
    "        lst_subreddit = []\n",
    "        for submit in submissions:\n",
    "            words = submit.title.split()\n",
    "            cashtags = list(set(filter(lambda word: word.lower().startswith('$'), words)))\n",
    "            if  len(cashtags) > 0:\n",
    "                val = RedditData.has_numbers(str(cashtags))\n",
    "                if  not val:\n",
    "                    tag_list.append(cashtags)\n",
    "                    comment_list.append(submit.title)\n",
    "                    date = dt.datetime.fromtimestamp(submit.created_utc)\n",
    "                    #pylint: disable=syntax-error\n",
    "                    date = date.replace(tzinfo=dt.timezone.utc).strftime(\"%m/%d/%Y %H\")\n",
    "                    date_list.append(date)\n",
    "                    subreddit_list.append(submit.subreddit)\n",
    "        length = len(tag_list)\n",
    "        for i in range(length):\n",
    "            if len(tag_list[i]) >= 1:\n",
    "                for j in range(len(tag_list[i])):\n",
    "                    clean_tag = RedditData.has_special_chars((tag_list[i][j]))\n",
    "                    lst_tag.append(clean_tag)\n",
    "                    lst_comments.append(comment_list[i])\n",
    "                    lst_date.append(date_list[i])\n",
    "                    lst_subreddit.append(subreddit_list[i])\n",
    "        return lst_tag, lst_comments, lst_date, lst_subreddit\n",
    "\n",
    "    @staticmethod\n",
    "    def add_to_df(tag_values, comment_values, date_values, subreddit_values, df_data):\n",
    "        '''\n",
    "        This function takes the list of tags, list of columns, subreddit list and the working\n",
    "        dataframe as inputs and appends each values to the corresponding rows.\n",
    "        '''\n",
    "        length_comment = len(comment_values)\n",
    "        for i in range(length_comment):\n",
    "            df_length = len(df_data)\n",
    "            df_data.loc[df_length] = date_values[i], comment_values[i], tag_values[i], subreddit_values[i]\n",
    "\n",
    "    @staticmethod\n",
    "    def has_special_chars(input_string):\n",
    "        '''\n",
    "        This function takes a string that is stripped of the $ sign from the ticker and\n",
    "        returns coded value. Codes:- 1: Valid Ticker (No special characters), -1:\n",
    "        Trailing Special Character(Requires removal of trailing character) and 0:\n",
    "        Invalid Ticker (Contains non-trailing special characters).\n",
    "        '''\n",
    "        sub_str = input_string[1:]\n",
    "        dollar_char = input_string[:1]\n",
    "        bool_result = RedditData.__have_special_chars(sub_str)\n",
    "        if bool_result:\n",
    "            sub_str = re.sub(r'\\W+', '', sub_str)\n",
    "        clean_string = dollar_char + sub_str\n",
    "        return clean_string.upper()\n",
    "    #pylint: disable=inconsistent-return-statements\n",
    "    @staticmethod\n",
    "    def __have_special_chars(sub_string):\n",
    "        '''\n",
    "        This is a helper function used inside the has_special_chars() function that takes\n",
    "        in a string as an input and returns boolean True if the passed string contains a\n",
    "        special character.\n",
    "        '''\n",
    "        regexp = re.compile('[^a-zA-Z]+')\n",
    "        if regexp.search(sub_string):\n",
    "            return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b870fa",
   "metadata": {},
   "source": [
    "#### Part 1. Import Libraries and initiate the local variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a47894d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from psaw import PushshiftAPI\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import reddit_scraper_module as rd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "redAPI = PushshiftAPI()\n",
    "filename = 'Reddit_Data.csv'\n",
    "\n",
    "stocks = ['$FB','$AMZN','$AAPL','$NFLX','$GOOG']\n",
    "subreddits = ['wallstreetbets','RobinHood','Stocks','investing','StockMarket']\n",
    "#\n",
    "#df_main = pd.DataFrame(columns = ['Date','Comment', 'Tags','subreddit'])\n",
    "#df_main.rea\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec6e516",
   "metadata": {},
   "source": [
    "#### 1.1 Function to identify the Dates for Scrapping Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee53fad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def returndates(df):\n",
    "    before = datetime.now()\n",
    "    after = datetime(2021, 1, 1)\n",
    "    if df['Date'].count() > 0:\n",
    "        print('1')\n",
    "        after =max(pd.to_datetime(df[\"Date\"],format='%m/%d/%Y %H'))\n",
    "    before = int(before.timestamp())\n",
    "    after = int(after.timestamp())\n",
    "    return before, after"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3f7333",
   "metadata": {},
   "source": [
    "#### 1.2 Scrap Reddit Data for identified Subreddits and write data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61d127dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wallstreetbets\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2004/813874195.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mbefore\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mafter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mdf_reddit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_main\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         searchposts = list(redAPI.search_submissions(after=after, before=before,\n\u001b[0m\u001b[0;32m     12\u001b[0m                                     \u001b[0msubreddit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubreddit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                                     \u001b[0mfilter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'url'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'author'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'title'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'subreddit'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\psaw\\PushshiftAPI.py\u001b[0m in \u001b[0;36m_search\u001b[1;34m(self, kind, stop_condition, return_batch, dataset, **kwargs)\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[0mendpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'{dataset}/{kind}/search'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mresponse\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_paging\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m'aggs'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'aggs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\psaw\\PushshiftAPI.py\u001b[0m in \u001b[0;36m_handle_paging\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    213\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_nec_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpayload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpayload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlimit\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\psaw\\PushshiftAPI.py\u001b[0m in \u001b[0;36m_get\u001b[1;34m(self, url, payload)\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unable to connect to pushshift.io. Retrying after backoff.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impose_rate_limit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m             \u001b[0mi\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\psaw\\PushshiftAPI.py\u001b[0m in \u001b[0;36m_impose_rate_limit\u001b[1;34m(self, nth_request)\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minterval\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m             \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Imposing rate limit, sleeping for %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0minterval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minterval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_add_nec_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpayload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#after = int(datetime(2022, 4, 1).timestamp())\n",
    "#before = int(datetime(2022, 5, 4).timestamp())\n",
    "warnings.filterwarnings('ignore')\n",
    "for subreddit in subreddits:\n",
    "    print(subreddit)\n",
    "    df_main = pd.read_csv(filename)\n",
    "    before, after = returndates(df_main[df_main[\"Subreddit\"] == subreddit])\n",
    "    \n",
    "    while before > after:\n",
    "        df_reddit = pd.DataFrame(columns = df_main.columns) \n",
    "        searchposts = list(redAPI.search_submissions(after=after, before=before,\n",
    "                                    subreddit=subreddit,\n",
    "                                    filter=['url','author', 'title', 'subreddit'],\n",
    "                                    limit=10000))\n",
    "        tag, comment, date,sreddit = rd.RedditData.cashtags(searchposts)\n",
    "        if len(tag) > 0:\n",
    "            print(datetime.fromtimestamp(before))\n",
    "            rd.RedditData.add_to_df(tag, comment, date, sreddit,df_reddit)\n",
    "            b1  = min(pd.to_datetime(df_reddit[\"Date\"],format='%m/%d/%Y %H'))\n",
    "            df_reddit = df_reddit[df_reddit[\"Tags\"].isin(stocks)]\n",
    "            if df_reddit['Date'].count() > 0:\n",
    "                print(df_reddit['Date'].count())\n",
    "                df_reddit.to_csv(filename,header=False,index=False,mode='a' )\n",
    "#            else:\n",
    "#                b1 = datetime.fromtimestamp(before)-timedelta(days=1)    \n",
    "        else:\n",
    "            #b1 = datetime.fromtimestamp(before)-timedelta(days=1)\n",
    "            b1 = datetime.fromtimestamp(after)\n",
    "        before = int(b1.timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac01ba9f",
   "metadata": {},
   "source": [
    "### This section is followed Reddit_TradeX_Sentiment.py for use of the data scrapped for TradeX to derive sentiment from Reddit submissions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
